{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "horizontal-delicious",
   "metadata": {},
   "source": [
    "# Compare Economic Class ENEM 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "interesting-original",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from CompareGroupsFunctions import *\n",
    "\n",
    "ano = '2019'\n",
    "gp_feat = 'CLASSE'\n",
    "gp_name = 'classe'\n",
    "gp_map = {'E': 'Classe E',\n",
    "          'D': 'Classe D',\n",
    "          'C': 'Classe C',\n",
    "          'B': 'Classe B',\n",
    "          'A': 'Classe A'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outstanding-sensitivity",
   "metadata": {},
   "source": [
    "## 1. Ciências Humanas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "freelance-count",
   "metadata": {},
   "outputs": [],
   "source": [
    "ch_comp = GroupComparator(comp='CH', ano=ano, gp_feat=gp_feat, gp_name=gp_name, gp_map=gp_map)\n",
    "df_gp = ch_comp.get_df_gp()\n",
    "ch_comp.print_mean_std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "realistic-firewall",
   "metadata": {},
   "outputs": [],
   "source": [
    "ch_comp.plot_hist_scores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cleared-conservation",
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_ch = ch_comp.bin_scores(nota_min=300, nota_max=800, step=20)\n",
    "auc_ch = ch_comp.auc_groups()\n",
    "sorted_auc_dif_ch = ch_comp.sort_abs_auc_dif()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "marked-browse",
   "metadata": {},
   "outputs": [],
   "source": [
    "ch_comp.plot_item_compar(item = 'Item 118067')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wrapped-history",
   "metadata": {},
   "outputs": [],
   "source": [
    "ch_comp.plot_item_compar(item = 'Item 118054')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "running-earth",
   "metadata": {},
   "source": [
    "## 2. Ciências da Natureza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "finnish-poison",
   "metadata": {},
   "outputs": [],
   "source": [
    "cn_comp = GroupComparator(comp='CN', ano=ano, gp_feat=gp_feat, gp_name=gp_name, gp_map=gp_map)\n",
    "df_gp = cn_comp.get_df_gp()\n",
    "cn_comp.print_mean_std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fluid-argentina",
   "metadata": {},
   "outputs": [],
   "source": [
    "cn_comp.plot_hist_scores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opening-staff",
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_cn = cn_comp.bin_scores(nota_min=300, nota_max=760, step=20)\n",
    "auc_cn = cn_comp.auc_groups()\n",
    "sorted_auc_dif_cn = cn_comp.sort_abs_auc_dif()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bright-worry",
   "metadata": {},
   "outputs": [],
   "source": [
    "cn_comp.plot_item_compar(item = 'Item 117868')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subsequent-century",
   "metadata": {},
   "outputs": [],
   "source": [
    "cn_comp.plot_item_compar(item = 'Item 117875')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interested-prayer",
   "metadata": {},
   "source": [
    "## 3. Matemática"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "concrete-adobe",
   "metadata": {},
   "outputs": [],
   "source": [
    "mt_comp = GroupComparator(comp='MT', ano=ano, gp_feat=gp_feat, gp_name=gp_name, gp_map=gp_map)\n",
    "df_gp = mt_comp.get_df_gp()\n",
    "mt_comp.print_mean_std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "white-visitor",
   "metadata": {},
   "outputs": [],
   "source": [
    "mt_comp.plot_hist_scores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latest-chance",
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_mt = mt_comp.bin_scores(nota_min=360, nota_max=800, step=20)\n",
    "auc_mt = mt_comp.auc_groups()\n",
    "sorted_auc_dif_mt = mt_comp.sort_abs_auc_dif()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pretty-watts",
   "metadata": {},
   "outputs": [],
   "source": [
    "mt_comp.plot_item_compar(item = 'Item 117726')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "endless-small",
   "metadata": {},
   "outputs": [],
   "source": [
    "mt_comp.plot_item_compar(item = 'Item 78445')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "environmental-silicon",
   "metadata": {},
   "outputs": [],
   "source": [
    "mt_comp.plot_item_compar(item = 'Item 83994')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "local-artist",
   "metadata": {},
   "source": [
    "## 4. Linguagens e Códigos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aerial-organ",
   "metadata": {},
   "outputs": [],
   "source": [
    "comp = 'LC'\n",
    "\n",
    "lc_comp = GroupComparator(comp=comp, ano=ano, gp_feat=gp_feat, gp_name=gp_name, gp_map=gp_map)\n",
    "df_gp = lc_comp.get_df_gp()\n",
    "\n",
    "item_ing = ['Item 55166', 'Item 54110', 'Item 23564', 'Item 31954', 'Item 118180']\n",
    "item_esp = ['Item 96705', 'Item 118222','Item 39016', 'Item 118167', 'Item 48558']\n",
    "\n",
    "df_gp_ing = {}\n",
    "df_gp_esp = {}\n",
    "df_gp_pt = {}\n",
    "\n",
    "for g in df_gp.keys():\n",
    "    df_gp_ing[g] = df_gp[g][df_gp[g]['TP_LINGUA']==0][['NU_INSCRICAO','NU_NOTA_LC']].copy()\n",
    "    df_gp_esp[g] = df_gp[g][df_gp[g]['TP_LINGUA']==1][['NU_INSCRICAO','NU_NOTA_LC']].copy()\n",
    "    df_gp_pt[g] = df_gp[g][['NU_INSCRICAO','NU_NOTA_LC']].copy()\n",
    "    \n",
    "    for col in df_gp[list(df_gp.keys())[0]].columns:\n",
    "        if col in item_ing:\n",
    "            df_gp_ing[g][col] = df_gp[g][df_gp[g]['TP_LINGUA']==0][col]\n",
    "        elif col in item_esp:\n",
    "            df_gp_esp[g][col] = df_gp[g][df_gp[g]['TP_LINGUA']==1][col]\n",
    "        elif col!='NU_INSCRICAO' and col!='NU_NOTA_LC' and col!='TP_LINGUA':\n",
    "            df_gp_pt[g][col] = df_gp[g][col]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "german-testimony",
   "metadata": {},
   "source": [
    "### 4.1 Inglês"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "injured-stage",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Proporção de participantes em inglês:\")\n",
    "for g in df_gp.keys():\n",
    "    if g=='Geral':\n",
    "        print(f\"{g}: {1-df_gp[g].mean()['TP_LINGUA']:.3f}\")\n",
    "    else:\n",
    "        print(f\"{lc_comp.gp_map[g]}: {1-df_gp[g].mean()['TP_LINGUA']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "therapeutic-meeting",
   "metadata": {},
   "outputs": [],
   "source": [
    "lc_comp_ing = GroupComparator(comp=comp, ano=ano, gp_feat=gp_feat, gp_name=gp_name, gp_map=gp_map)\n",
    "lc_comp_ing.df_gp = df_gp_ing\n",
    "lc_comp_ing.print_mean_std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "owned-scroll",
   "metadata": {},
   "outputs": [],
   "source": [
    "lc_comp_ing.plot_hist_scores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subject-cathedral",
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_lc_ing = lc_comp_ing.bin_scores(nota_min=350, nota_max=750)\n",
    "auc_lc_ing = lc_comp_ing.auc_groups()\n",
    "sorted_auc_dif_lc_ing = lc_comp_ing.sort_abs_auc_dif()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "popular-cambodia",
   "metadata": {},
   "outputs": [],
   "source": [
    "lc_comp_ing.plot_item_compar(item = 'Item 118180')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sensitive-theater",
   "metadata": {},
   "outputs": [],
   "source": [
    "lc_comp_ing.plot_item_compar(item = 'Item 23564')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "religious-power",
   "metadata": {},
   "outputs": [],
   "source": [
    "lc_comp_ing.plot_item_compar(item = 'Item 54110')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ethical-nursery",
   "metadata": {},
   "outputs": [],
   "source": [
    "lc_comp_ing.plot_item_compar(item = 'Item 31954')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "right-reynolds",
   "metadata": {},
   "source": [
    "### 4.2 Espanhol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "golden-trail",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Proporção de participantes em espanhol:\")\n",
    "for g in df_gp.keys():\n",
    "    if g=='Geral':\n",
    "        print(f\"{g}: {df_gp[g].mean()['TP_LINGUA']:.3f}\")\n",
    "    else:\n",
    "        print(f\"{lc_comp.gp_map[g]}: {df_gp[g].mean()['TP_LINGUA']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "korean-sector",
   "metadata": {},
   "outputs": [],
   "source": [
    "lc_comp_esp = GroupComparator(comp=comp, ano=ano, gp_feat=gp_feat, gp_name=gp_name, gp_map=gp_map)\n",
    "lc_comp_esp.df_gp = df_gp_esp\n",
    "lc_comp_esp.print_mean_std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acknowledged-disorder",
   "metadata": {},
   "outputs": [],
   "source": [
    "lc_comp_esp.plot_hist_scores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "established-motel",
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_lc_esp = lc_comp_esp.bin_scores(nota_min=350, nota_max=750)\n",
    "auc_lc_esp = lc_comp_esp.auc_groups()\n",
    "sorted_auc_dif_lc_esp = lc_comp_esp.sort_abs_auc_dif()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collectible-clearing",
   "metadata": {},
   "outputs": [],
   "source": [
    "lc_comp_esp.plot_item_compar(item = 'Item 118222')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "front-denmark",
   "metadata": {},
   "source": [
    "### 4.3 Português"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "white-saturn",
   "metadata": {},
   "outputs": [],
   "source": [
    "lc_comp_pt = GroupComparator(comp=comp, ano=ano, gp_feat=gp_feat, gp_name=gp_name, gp_map=gp_map)\n",
    "lc_comp_pt.df_gp = df_gp_pt\n",
    "lc_comp_pt.print_mean_std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "color-income",
   "metadata": {},
   "outputs": [],
   "source": [
    "lc_comp_pt.plot_hist_scores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifty-closing",
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_lc_pt = lc_comp_pt.bin_scores(nota_min=350, nota_max=750)\n",
    "auc_lc_pt = lc_comp_pt.auc_groups()\n",
    "sorted_auc_dif_lc_pt = lc_comp_pt.sort_abs_auc_dif()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "usual-connection",
   "metadata": {},
   "outputs": [],
   "source": [
    "lc_comp_pt.plot_item_compar(item = 'Item 81992')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "similar-retreat",
   "metadata": {},
   "outputs": [],
   "source": [
    "lc_comp_pt.plot_item_compar(item = 'Item 118103')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electronic-artwork",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
